{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification accuracy\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "* A machine learning algorithm has been trained to predict whether or not it would rain the next day. \n",
    "* Out of 365 predictions, it got 300 correct, compute the accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = 365\n",
    "correct_predictions = 300\n",
    "\n",
    "accuracy = correct_predictions/predictions\n",
    "accuracy_percentage = accuracy * 100\n",
    "\n",
    "print(f\"The machine learning algorithm's accuracy is {accuracy:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "A machine learning model has been trained to detect fire. Here is the result of its predictions:\n",
    "\n",
    "#### True Positive (TP)\n",
    "* Reality: fire\n",
    "* Predicted: fire\n",
    "* Number of TP: 2\n",
    "\n",
    "#### False Positive (FP)\n",
    "* Reality: no fire\n",
    "* Predicted: fire\n",
    "* Number of FP: 2\n",
    "\n",
    "#### False Negative (FN)\n",
    "* Reality: fire\n",
    "* Predicted: no fire\n",
    "* Number of FN: 11\n",
    "\n",
    "#### True Negative (TN)\n",
    "* Reality: no fire\n",
    "* Predicted: no fire\n",
    "* Number of TN: 985\n",
    "\n",
    "Calculated the accuracy using the following formula:\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Is this a good model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 2\n",
    "true_negative = 985\n",
    "false_positive = 2\n",
    "false_negative = 11\n",
    "\n",
    "accurate_predictions = true_positive + true_negative\n",
    "total_number_of_predictions = true_positive + true_negative + false_positive + false_negative\n",
    "\n",
    "accuracy = accurate_predictions / total_number_of_predictions\n",
    "\n",
    "number_of_fires = true_positive + false_negative\n",
    "number_of_no_fires = false_positive + true_negative\n",
    "\n",
    "fire_accuracy = number_of_fires / total_number_of_predictions\n",
    "no_fire_accuracy = number_of_no_fires / total_number_of_predictions\n",
    "\n",
    "print(f\"There were a total of {total_number_of_predictions} predictions.\")\n",
    "print(f\"Out of those {total_number_of_predictions}, only {number_of_fires} of them were fires, and {number_of_no_fires} no fires.\")\n",
    "print(f\"The accuracy of the model is {accuracy:.3f}. However, it's accuracy in predicting fire is only {fire_accuracy:.3f}, while it's accuracy in predicting no fire is {no_fire_accuracy:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The accuracy of the model is high, but I see a few problems:\n",
    "\n",
    "* The accuracy is so high because the number of fires is so small in comparison with the number of no fires. \n",
    "* The results aren't nearly as good as the accuracy formula indicates. There were 13 fires in total, but the model only predicted 2 of them. In contrast, it was able to predict 985 out of 987 no fires.\n",
    "* Since the model's primary purpose is to detect fires, I'd say it's quite bad. Then again, predicting 2 fires means that there were 2 times where firemen had time to prepare, lowering the risk of injuries and the risk of fire spreading. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
